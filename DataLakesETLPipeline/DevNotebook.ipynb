{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import (\n",
    "    udf, \n",
    "    col, \n",
    "    year, \n",
    "    month, \n",
    "    dayofmonth, \n",
    "    hour,\n",
    "    weekofyear, \n",
    "    date_format, \n",
    "    dayofweek, \n",
    "    monotonically_increasing_id,\n",
    "    max\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, \n",
    "    StructField as Fld, \n",
    "    StringType as Str, \n",
    "    DoubleType as Dbl, \n",
    "    IntegerType as Int, \n",
    "    TimestampType as Ts\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['REGION']=config['AWS']['REGION']\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['SECRET_ACCESS_KEY']\n",
    "os.environ['BUCKET']=config['AWS']['BUCKET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Function to initiate a Spark session and return the resulting object.\n",
    "    \n",
    "    Returns: SparkSession object\n",
    "\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark.conf.set(\"spark.sql.parquet.compression.codec\", \"gzip\")\n",
    "    spark.conf.set(\"spark.executor.extraJavaOptions\",\"-Dcom.amazonaws.services.s3.enableV4=true\")\n",
    "    spark.conf.set(\"spark.driver.extraJavaOptions\",\"-Dcom.amazonaws.services.s3.enableV4=true\")\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_s3_bucket(acl=\"private\"):\n",
    "    \"\"\"\n",
    "    Create S3 bucket in AWS region\n",
    "    \n",
    "    Args:\n",
    "        acl: access control level for the bucket being created - set it to public to be accessible from anywhere\n",
    "    \n",
    "    Returns: created bucket/None\n",
    "    \n",
    "    \"\"\"\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError\n",
    "    \n",
    "    try:\n",
    "        if os.environ.get('REGION', None) is None:\n",
    "            client = boto3.client('s3')\n",
    "            client.create_bucket(Bucket=os.environ.get('BUCKET'))\n",
    "        else:\n",
    "            client = boto3.client('s3', region_name=os.environ.get('REGION'))\n",
    "            loc = {'LocationConstraint': os.environ.get('REGION')}\n",
    "            client.create_bucket(\n",
    "                Bucket=os.environ.get('BUCKET'),\n",
    "                CreateBucketConfiguration=loc,\n",
    "                ACL=acl\n",
    "            )\n",
    "    except client.exceptions.BucketAlreadyOwnedByYou:\n",
    "        pass\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        raise e\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    TL;DR: Read raw song data in S3 - transform - store as analytics data in S3\n",
    "    \n",
    "    Function to read in JSON data from an S3 bucket containing song data for the Sparkify \n",
    "    streaming service; define an appropriate analytics schema for songs and artists;\n",
    "    extract the raw data into the defined schemas; store the resulting data as\n",
    "    parquet files on S3. \n",
    "    \n",
    "    Args:\n",
    "        spark: Spark session object\n",
    "        input_data: input S3 bucket string\n",
    "        output_data: output S3 bucket string\n",
    "\n",
    "    Returns: Parquet data stored in output_data directory / None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get filepath to song data file\n",
    "    song_data = input_data + \"song_data/*/*/*/*.json\"\n",
    "    \n",
    "    # create song_data schema\n",
    "    schema__song_data = StructType([\n",
    "        # StructField(field_name, field_type, nullable_boolean)\n",
    "        Fld(\"artist_id\", Str(), False),\n",
    "        Fld(\"artist_latitude\", Str(), True),\n",
    "        Fld(\"artist_longitude\", Str(), True),\n",
    "        Fld(\"artist_location\", Str(), True),\n",
    "        Fld(\"artist_name\", Str(), False),\n",
    "        Fld(\"song_id\", Str(), False),\n",
    "        Fld(\"title\", Str(), False),\n",
    "        Fld(\"duration\", Dbl(), False),\n",
    "        Fld(\"year\", Int(), False)\n",
    "    ])\n",
    "    \n",
    "    # read song data file\n",
    "    song_df = spark.read.json(song_data, schema=schema__song_data)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    process_songs(song_df, spark, input_data, output_data)\n",
    "        \n",
    "    # extract columns to create artists table\n",
    "    process_artists(song_df, spark, input_data, output_data)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    TL;DR: Read raw log data in S3 - transform - store as analytics data in S3\n",
    "    \n",
    "    Function to read in JSON data from an S3 bucket containing logs data for the Sparkify \n",
    "    streaming service; define an appropriate analytics schema for time, users, and songplays;\n",
    "    extract the raw data into the defined schemas; store the resulting data as\n",
    "    parquet files on S3. \n",
    "    \n",
    "    Args:\n",
    "        spark: Spark session object\n",
    "        input_data: input S3 bucket string\n",
    "        output_data: output S3 bucket string\n",
    "\n",
    "    Returns: Parquet data stored in output_data directory / None\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # get filepath to log data file\n",
    "    log_data = input_data + \"log_data/*/*/*.json\"\n",
    "    \n",
    "    # create log_data schema\n",
    "    schema__log_data = StructType([\n",
    "        # StructField(field_name, field_type, nullable_boolean)\n",
    "        Fld(\"artist\", Str(), True),\n",
    "        Fld(\"auth\", Str(), False),\n",
    "        Fld(\"firstName\", Str(), True),\n",
    "        Fld(\"gender\", Str(), True),\n",
    "        Fld(\"itemInSession\", Int(), False),\n",
    "        Fld(\"lastName\", Str(), True),\n",
    "        Fld(\"length\", Dbl(), True),\n",
    "        Fld(\"level\", Str(), False),\n",
    "        Fld(\"location\", Str(), True),\n",
    "        Fld(\"method\", Str(), False),\n",
    "        Fld(\"page\", Str(), False),\n",
    "        Fld(\"registration\", Dbl(), True),\n",
    "        Fld(\"sessionId\", Int(), False),\n",
    "        Fld(\"song\", Str(), True),\n",
    "        Fld(\"status\", Int(), False),\n",
    "        Fld(\"ts\", Dbl(), False),\n",
    "        Fld(\"userAgent\", Str(), True),\n",
    "        Fld(\"userId\", Str(), True)\n",
    "    ])\n",
    "\n",
    "    # read log data file\n",
    "    log_df = spark.read.json(log_data, schema=schema__log_data)\n",
    "    \n",
    "    log_df.printSchema()\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    log_df = log_df.filter(col(\"page\") == \"NextSong\" )\n",
    "\n",
    "    # extract columns for users table\n",
    "    process_users(log_df, spark, input_data, output_data)\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    process_time(log_df, spark, input_data, output_data)\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    process_songplays(log_df, spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_songs(df, spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Function to extract song data from the given dataframe and store it into parquet files on S3.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark dataframe object\n",
    "        spark: Spark session object\n",
    "        input_data: input S3 bucket string\n",
    "        output_data: output S3 bucket string\n",
    "\n",
    "    Returns: Parquet data stored in S3\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    songs_table = df.select(\n",
    "        \"song_id\", \n",
    "        \"title\", \n",
    "        \"artist_id\", \n",
    "        \"year\", \n",
    "        \"duration\"\n",
    "    )\n",
    "        \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    try:\n",
    "        songs_table.write.mode(\n",
    "            'overwrite'\n",
    "        ).partitionBy(\n",
    "            'year', 'artist_id'\n",
    "        ).parquet(\n",
    "            output_data + \"songs/songs_table.parquet\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        raise e\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_artists(df, spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Function to extract artist data from the given dataframe and store it into parquet files on S3.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark dataframe object\n",
    "        spark: Spark session object\n",
    "        input_data: input S3 bucket string\n",
    "        output_data: output S3 bucket string\n",
    "\n",
    "    Returns: Parquet data stored in S3\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    artists_table = df.select(\n",
    "            \"artist_id\",\n",
    "            col(\"artist_name\").alias(\"name\"),\n",
    "            col(\"artist_location\").alias(\"location\"),\n",
    "            col(\"artist_latitude\").alias(\"latitude\"),\n",
    "            col(\"artist_longitude\").alias(\"longitude\")\n",
    "    ).distinct()\n",
    "        \n",
    "    # write artists table to parquet files\n",
    "    try:\n",
    "        artists_table.write.mode(\n",
    "            'overwrite'\n",
    "        ).parquet(\n",
    "            output_data + \"artists/artists_table.parquet\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        raise e\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_users(df, spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Function to extract user data from the given dataframe and store it into parquet files on S3.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark dataframe object\n",
    "        spark: Spark session object\n",
    "        input_data: input S3 bucket string\n",
    "        output_data: output S3 bucket string\n",
    "\n",
    "    Returns: Parquet data stored in S3\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # to get only applicable rows: \n",
    "    #     - aggregate ts by user to find last activity date\n",
    "    #     - filter where ts = max_ts & user_id != \"\" & user_id is not null\n",
    "    users_table = df.withColumn(\n",
    "        \"max_ts\", fn.max(\"ts\").over(Window.partitionBy(\"userId\"))\n",
    "    ).filter(\n",
    "        (\n",
    "            (col(\"ts\") == col(\"max_ts\")) & (col(\"userId\") != \"\") & (col(\"userId\").isNotNull())\n",
    "        )\n",
    "    ).select(\n",
    "        col(\"userID\").alias(\"id\"),\n",
    "        col(\"firstName\").alias(\"first_name\"),\n",
    "        col(\"lastName\").alias(\"last_name\"),\n",
    "        \"gender\",\n",
    "        \"level\"\n",
    "    )\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    try:\n",
    "        users_table.write.mode(\n",
    "            'overwrite'\n",
    "        ).parquet(\n",
    "            output_data + \"users/users_table.parquet\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        raise e\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_time(df, spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Function to extract time data from the given dataframe and store it into parquet files on S3.\n",
    "\n",
    "    Args:\n",
    "        df: Spark dataframe object\n",
    "        spark: Spark session object\n",
    "        input_data: input S3 bucket string\n",
    "        output_data: output S3 bucket string\n",
    "\n",
    "    Returns: Parquet data stored in S3\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    get_timestamp = udf(\n",
    "        lambda x: datetime.fromtimestamp(x / 1000).replace(microsecond=0),\n",
    "        Ts()\n",
    "    )\n",
    "    df = df.withColumn(\"start_time\", get_timestamp(\"ts\"))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df.withColumn(\n",
    "        \"hour\", hour(\"start_time\")\n",
    "    ).withColumn(\n",
    "        \"day\", dayofmonth(\"start_time\")\n",
    "    ).withColumn(\n",
    "        \"week\", weekofyear(\"start_time\")\n",
    "    ).withColumn(\n",
    "        \"month\", month(\"start_time\")\n",
    "    ).withColumn(\n",
    "        \"year\", year(\"start_time\")\n",
    "    ).withColumn(\n",
    "        \"weekday\", dayofweek(\"start_time\")\n",
    "    ).select(\n",
    "        \"start_time\", \n",
    "        \"hour\", \n",
    "        \"day\", \n",
    "        \"week\", \n",
    "        \"month\", \n",
    "        \"year\", \n",
    "        \"weekday\"\n",
    "    ).distinct()\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    try:\n",
    "        time_table.write.mode(\n",
    "            'overwrite'\n",
    "        ).partitionBy(\n",
    "            'year', 'month'\n",
    "        ).parquet(\n",
    "            output_data + \"time/time_table.parquet\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        raise e\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_songplays(df, spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Function to extract songplay data from the given dataframe and store it into parquet files on S3.\n",
    "\n",
    "    Args:\n",
    "        df: Spark dataframe object\n",
    "        spark: Spark session object\n",
    "        input_data: input S3 bucket string\n",
    "        output_data: output S3 bucket string\n",
    "\n",
    "    Returns: Parquet data stored in S3\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    songs = spark.read.parquet(output_data + 'songs/songs_table.parquet')\n",
    "    \n",
    "    # read in artist data to use for songplays table\n",
    "    artists = spark.read.parquet(output_data + 'artists/artists_table.parquet')\n",
    "    \n",
    "    # read in time data to use for songplays table\n",
    "    time_table = spark.read.parquet(output_data + 'time/time_table.parquet')\n",
    "\n",
    "    # create a table joining songs and artists\n",
    "    artists_songs = songs.join(\n",
    "        artists, \"artist_id\", \"full\"\n",
    "    ).select(\n",
    "        \"song_id\", \"title\", \"artist_id\", \"name\", \"duration\"\n",
    "    )\n",
    "    \n",
    "    # extract columns from joined song and log datasets to create songplays table\n",
    "    # inflate logs dataframe with song/artist data by left joining artists_songs\n",
    "    songplays_table = df.join(\n",
    "        artists_songs,\n",
    "        [\n",
    "            df.song == songs.title,\n",
    "            df.artist == songs.name,\n",
    "            df.length == songs.duration\n",
    "        ],\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # inflate the new songplays dataframe with the time table data\n",
    "    # select the final table schema columns from the joined dataframes\n",
    "    # create a self-incrementing songplay ID column\n",
    "    songplays_table = songplays_table.join(\n",
    "            time_table, \"start_time\", \"left\"\n",
    "        ).select(\n",
    "            \"start_time\",\n",
    "            col(\"userId\").alias(\"user_id\"),\n",
    "            \"level\",\n",
    "            \"song_id\",\n",
    "            \"artist_id\",\n",
    "            col(\"sessionId\").alias(\"session_id\"),\n",
    "            \"location\",\n",
    "            col(\"userAgent\").alias(\"user_agent\"),\n",
    "            \"year\",\n",
    "            \"month\"\n",
    "        ).withColumn(\n",
    "            \"songplay_id\", monotonically_increasing_id()\n",
    "        )\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    try:\n",
    "        songplays_table.write.mode(\n",
    "            'overwrite'\n",
    "        ).partitionBy(\n",
    "            'year', 'month'\n",
    "        ).parquet(\n",
    "            output_data + \"songplays/songplays_table.parquet\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        raise e\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Function to kick off ETL process.\n",
    "\n",
    "    Args:\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    create_s3_bucket(acl=\"public-read\")\n",
    "    \n",
    "#     # S3\n",
    "#     input_data = \"s3a://udacity-dend/\"\n",
    "#     output_data = f\"s3a://{os.environ.get('BUCKET')}/\"\n",
    "    \n",
    "    # Local\n",
    "    input_data = \"data/input/\"\n",
    "    output_data = \"data/output/\"\n",
    "    \n",
    "    process_song_data(spark, input_data, output_data)\n",
    "    process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
